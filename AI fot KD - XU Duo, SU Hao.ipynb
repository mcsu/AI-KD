{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projet d'AIKD\n",
    "# XU Duo, SU Hao\n",
    "\n",
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 20 news\n"
     ]
    }
   ],
   "source": [
    "# get 30 news\n",
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key='888d5de05649432887e6499926707525')\n",
    "\n",
    "all_articles = newsapi.get_everything(\n",
    "                                      \n",
    "                                      from_param='2018-11-15',\n",
    "                                      to='2018-11-20',\n",
    "                                      language='en',\n",
    "                                      domains='bbc.com, bbc.co.uk '\n",
    "                                      )\n",
    "num_news = len(all_articles['articles'])\n",
    "print(\"get {} news\".format(num_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_X = []\n",
    "test_news_date = []\n",
    "for i in range(20):\n",
    "    test_news_X.append(all_articles['articles'][i]['content'])\n",
    "    test_news_date.append(all_articles['articles'][i]['publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# get train dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word process\n",
    "stop_words_I = []\n",
    "with open('/Users/haosu/Desktop/停词.txt') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        stop_words_I.append(line)\n",
    "        line = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "def stop_word_process(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataBank = []\n",
    "    #Separate words\n",
    "    for article in text:\n",
    "        dataBank.append(word_tokenize(article))\n",
    "    stop_words.add(\"I\")\n",
    "    stop_words.add(\",\")\n",
    "    stop_words.add(\"(\")\n",
    "    stop_words.add(\")\")\n",
    "    stop_words.add(\"-\")\n",
    "    stop_words.add(\"_\")\n",
    "    stop_words.add(\".\")\n",
    "    stop_words.add(\":\")\n",
    "    stop_words.add(\"A\")\n",
    "    stop_words.add(\"<\")\n",
    "    stop_words.add(\">\")\n",
    "    stop_words.add(\"n't\")\n",
    "    stop_words.add(\"'s\")\n",
    "    stop_words.add(\"?\")\n",
    "    stop_words.add(\"!\")\n",
    "    stop_words.add(\"--\")\n",
    "    stop_words.add(\"chars\")\n",
    "    for i in range(len(stop_words_I)):\n",
    "        stop_words.add(stop_words_I[i])\n",
    "    # NLTK process\n",
    "    twenty_train_split = []\n",
    "    for i in range(len(dataBank)):\n",
    "        filtered_sentence = []\n",
    "        for word in dataBank[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        twenty_train_split.append(filtered_sentence)\n",
    "    #twenty_train_split 是去除停词后的文本\n",
    "    su = []\n",
    "    for i in range(len(twenty_train_split)):\n",
    "        su.append(\" \".join(twenty_train_split[i]))\n",
    "    return su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train_filtered = stop_word_process(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNULL(text):\n",
    "    # remove the null values\n",
    "    for i in range(len(text)):\n",
    "        if test_news_X[i] == None :\n",
    "            #test_news_X.remove(text[i])\n",
    "            test_news_X.pop(i)\n",
    "            test_news_date.pop(i)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_news_X)):\n",
    "    if isinstance(test_news_X[i],str):\n",
    "        print(\"OK\")  \n",
    "    else: \n",
    "        print(\"NoneType\")\n",
    "        test_news_X = removeNULL(test_news_X)\n",
    "thirty_test_filtered = stop_word_process(test_news_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9049849743680396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train_filtered)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train_filtered, twenty_train.target)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train_filtered, twenty_train.target)\n",
    "print(gs_clf.best_score_)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.guns\n",
      "talk.politics.guns\n",
      "rec.sport.hockey\n",
      "rec.motorcycles\n",
      "rec.sport.hockey\n",
      "sci.space\n",
      "rec.motorcycles\n",
      "talk.politics.misc\n",
      "sci.med\n",
      "sci.med\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "talk.politics.misc\n",
      "rec.sport.hockey\n",
      "talk.politics.guns\n",
      "Best accuracy：0.9049849743680396\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "X_new_counts = count_vect.transform(thirty_test_filtered)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = gs_clf.predict(thirty_test_filtered)\n",
    "predict_result = []\n",
    "for i in predicted:\n",
    "    print(twenty_train.target_names[i])\n",
    "    predict_result.append(twenty_train.target_names[i])\n",
    "print(\"Best accuracy：%r\" % (gs_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#test_news_X\n",
    "#test_news_date\n",
    "#predict_result\n",
    "print(len(test_news_X))\n",
    "print(len(test_news_date))\n",
    "print(len(predict_result))\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame({'content':test_news_X,'date':test_news_date,'label':predict_result})\n",
    "dataframe.to_csv(\"/Users/haosu/Desktop/aikd.csv\",index=False,sep=',')\n",
    "#data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e6d7e960-f7e7-11e8-8ace-acde48001122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlist1 -> ID\\nlist3 -> label\\nlist5 -> time\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KD\n",
    "import uuid\n",
    "print(uuid.uuid1())\n",
    "'''\n",
    "list1 -> ID\n",
    "list3 -> label\n",
    "list5 -> time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "def create():\n",
    "    g = rdflib.Graph()\n",
    "    belongs_class_of = rdflib.URIRef('http://www.example.org/belongs_class_of')\n",
    "    published_at = rdflib.URIRef('http://www.example.org/published_at')\n",
    "    \n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]   \n",
    "    \n",
    "    for i in range(len(test_news_X)):\n",
    "        list1.append(rdflib.URIRef('http://www.example.org/'+\"article{}\".format(i+1)))\n",
    "    print(list1)\n",
    "   \n",
    "    for i in predict_result:\n",
    "       \n",
    "        list2.append(rdflib.URIRef('http://www.example.org/'+i))\n",
    "            \n",
    "    for i in test_news_date:\n",
    "        list3.append(rdflib.URIRef('http://www.example.org/'+i))\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "         g.add((list1[i],belongs_class_of,list2[i]))\n",
    "    for i in range(len(list1)):       \n",
    "         g.add((list1[i],published_at,list3[i]))\n",
    "    g.serialize(\"graph.rdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rdflib.term.URIRef('http://www.example.org/article1'), rdflib.term.URIRef('http://www.example.org/article2'), rdflib.term.URIRef('http://www.example.org/article3'), rdflib.term.URIRef('http://www.example.org/article4'), rdflib.term.URIRef('http://www.example.org/article5'), rdflib.term.URIRef('http://www.example.org/article6'), rdflib.term.URIRef('http://www.example.org/article7'), rdflib.term.URIRef('http://www.example.org/article8'), rdflib.term.URIRef('http://www.example.org/article9'), rdflib.term.URIRef('http://www.example.org/article10'), rdflib.term.URIRef('http://www.example.org/article11'), rdflib.term.URIRef('http://www.example.org/article12'), rdflib.term.URIRef('http://www.example.org/article13'), rdflib.term.URIRef('http://www.example.org/article14'), rdflib.term.URIRef('http://www.example.org/article15'), rdflib.term.URIRef('http://www.example.org/article16')]\n",
      "6 results found\n",
      "Result 1 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article15'),)\n",
      "Result 2 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article3'),)\n",
      "Result 3 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article5'),)\n",
      "Result 4 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article13'),)\n",
      "Result 5 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article12'),)\n",
      "Result 6 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article11'),)\n"
     ]
    }
   ],
   "source": [
    "def query():\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(\"graph.rdf\", format=\"xml\")\n",
    " \n",
    "    q = \"select ?article  where {?article <http://www.example.org/belongs_class_of>  <http://www.example.org/rec.sport.hockey> }\"\n",
    "    x = g.query(q)\n",
    "    t = list(x) \n",
    "    print(len(t),'results found')\n",
    "    \n",
    "    for i in range(len(t)):\n",
    "        print(\"Result {} of {} \".format(i+1,len(t)))\n",
    "        print(t[i])\n",
    "    # <?,b,?>\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    create()\n",
    "    query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "pyml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
