{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projet d'AIKD\n",
    "# XU Duo, SU Hao\n",
    "\n",
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 20 news\n"
     ]
    }
   ],
   "source": [
    "# get 30 news\n",
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key='888d5de05649432887e6499926707525')\n",
    "\n",
    "all_articles = newsapi.get_everything(\n",
    "                                      \n",
    "                                      from_param='2018-11-15',\n",
    "                                      to='2018-11-20',\n",
    "                                      language='en',\n",
    "                                      domains='bbc.com, bbc.co.uk '\n",
    "                                      )\n",
    "num_news = len(all_articles['articles'])\n",
    "print(\"get {} news\".format(num_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_X = []\n",
    "test_news_date = []\n",
    "for i in range(20):\n",
    "    test_news_X.append(all_articles['articles'][i]['content'])\n",
    "    test_news_date.append(all_articles['articles'][i]['publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# get train dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word process\n",
    "stop_words_I = []\n",
    "with open('/Users/haosu/Desktop/停词.txt') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        stop_words_I.append(line)\n",
    "        line = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "def stop_word_process(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataBank = []\n",
    "    #Separate words\n",
    "    for article in text:\n",
    "        dataBank.append(word_tokenize(article))\n",
    "    stop_words.add(\"I\")\n",
    "    stop_words.add(\",\")\n",
    "    stop_words.add(\"(\")\n",
    "    stop_words.add(\")\")\n",
    "    stop_words.add(\"-\")\n",
    "    stop_words.add(\"_\")\n",
    "    stop_words.add(\".\")\n",
    "    stop_words.add(\":\")\n",
    "    stop_words.add(\"A\")\n",
    "    stop_words.add(\"<\")\n",
    "    stop_words.add(\">\")\n",
    "    stop_words.add(\"n't\")\n",
    "    stop_words.add(\"'s\")\n",
    "    stop_words.add(\"?\")\n",
    "    stop_words.add(\"!\")\n",
    "    stop_words.add(\"--\")\n",
    "    stop_words.add(\"chars\")\n",
    "    for i in range(len(stop_words_I)):\n",
    "        stop_words.add(stop_words_I[i])\n",
    "    # NLTK process\n",
    "    twenty_train_split = []\n",
    "    for i in range(len(dataBank)):\n",
    "        filtered_sentence = []\n",
    "        for word in dataBank[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        twenty_train_split.append(filtered_sentence)\n",
    "    #twenty_train_split 是去除停词后的文本\n",
    "    su = []\n",
    "    for i in range(len(twenty_train_split)):\n",
    "        su.append(\" \".join(twenty_train_split[i]))\n",
    "    return su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train_filtered = stop_word_process(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNULL(text):\n",
    "    # remove the null values\n",
    "    for i in range(len(text)):\n",
    "        if test_news_X[i] == None :\n",
    "            #test_news_X.remove(text[i])\n",
    "            test_news_X.pop(i)\n",
    "            test_news_date.pop(i)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_news_X)):\n",
    "    if isinstance(test_news_X[i],str):\n",
    "        print(\"OK\")  \n",
    "    else: \n",
    "        print(\"NoneType\")\n",
    "        test_news_X = removeNULL(test_news_X)\n",
    "thirty_test_filtered = stop_word_process(test_news_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9049849743680396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train_filtered)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train_filtered, twenty_train.target)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train_filtered, twenty_train.target)\n",
    "print(gs_clf.best_score_)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.guns\n",
      "talk.politics.guns\n",
      "rec.sport.hockey\n",
      "rec.motorcycles\n",
      "rec.sport.hockey\n",
      "sci.space\n",
      "rec.motorcycles\n",
      "talk.politics.misc\n",
      "sci.med\n",
      "sci.med\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "rec.sport.hockey\n",
      "talk.politics.misc\n",
      "rec.sport.hockey\n",
      "talk.politics.guns\n",
      "Best accuracy：0.9049849743680396\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "X_new_counts = count_vect.transform(thirty_test_filtered)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = gs_clf.predict(thirty_test_filtered)\n",
    "predict_result = []\n",
    "for i in predicted:\n",
    "    print(twenty_train.target_names[i])\n",
    "    predict_result.append(twenty_train.target_names[i])\n",
    "print(\"Best accuracy：%r\" % (gs_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#test_news_X\n",
    "#test_news_date\n",
    "#predict_result\n",
    "print(len(test_news_X))\n",
    "print(len(test_news_date))\n",
    "print(len(predict_result))\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame({'content':test_news_X,'date':test_news_date,'label':predict_result})\n",
    "dataframe.to_csv(\"/Users/haosu/Desktop/aikd.csv\",index=False,sep=',')\n",
    "#data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20b11ea0-f7c9-11e8-b65c-acde48001122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlist1 -> ID\\nlist3 -> label\\nlist5 -> time\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KD\n",
    "import uuid\n",
    "print(uuid.uuid1())\n",
    "'''\n",
    "list1 -> ID\n",
    "list3 -> label\n",
    "list5 -> time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "def create():\n",
    "    g = rdflib.Graph()\n",
    "    belongs_class_of = rdflib.URIRef('http://www.example.org/belongs_class_of')\n",
    "    published_at = rdflib.URIRef('http://www.example.org/published_at')\n",
    "    \n",
    "    #sci_space = rdflib.URIRef('http://www.example.org/sci_space')\n",
    "    #soc_religion_christian = rdflib.URIRef('http://www.example.org/soc_religion_christian')\n",
    "    #talk_politics_mideast = rdflib.URIRef('http://www.example.org/talk_politics_mideast')\n",
    "    #talk_politics_guns = rdflib.URIRef('http://www.example.org/talk_politics_guns')\n",
    "    #talk_politics_misc = rdflib.URIRef('http://www.example.org/talk_politics_misc')\n",
    "    #sci_med = rdflib.URIRef('http://www.example.org/sci_med')\n",
    "    \n",
    "    #article1 = rdflib.URIRef('http://www.example.org/article1')\n",
    "    #article2 = rdflib.URIRef('http://www.example.org/article2')\n",
    "    #article3 = rdflib.URIRef('http://www.example.org/article3')\n",
    "    #article4 = rdflib.URIRef('http://www.example.org/article4')\n",
    "    #article5 = rdflib.URIRef('http://www.example.org/article5')\n",
    "    #article6 = rdflib.URIRef('http://www.example.org/article6')\n",
    "    #article7 = rdflib.URIRef('http://www.example.org/article7')\n",
    "    #article8 = rdflib.URIRef('http://www.example.org/article8')\n",
    "    #article9 = rdflib.URIRef('http://www.example.org/article9')\n",
    "    #article10 = rdflib.URIRef('http://www.example.org/article10')\n",
    "    #article11 = rdflib.URIRef('http://www.example.org/article11')\n",
    "    #article12 = rdflib.URIRef('http://www.example.org/article12')\n",
    "    #article13 = rdflib.URIRef('http://www.example.org/article13')\n",
    "    #article14 = rdflib.URIRef('http://www.example.org/article14')\n",
    "    #article15 = rdflib.URIRef('http://www.example.org/article15')\n",
    "    \n",
    "   \n",
    "    \n",
    "    #def switcher(var):\n",
    "     #   return {\n",
    "      #      'sci_space': fun1,\n",
    "       #     'soc_religion_christian': fun2,\n",
    "        #    'talk_politics_mideast': fun3,\n",
    "         #   'talk_politics_guns': fun4,\n",
    "          #  'talk_politics_misc': fun5,\n",
    "           # 'sci_med': fun6\n",
    "        #}.get(var,'error')    #'error'为默认返回值，可自设置'''\n",
    "    \n",
    "    list1=['article1',\n",
    "      'article2',\n",
    "      'article3',\n",
    "      'article4',\n",
    "      'article5',\n",
    "      'article6',\n",
    "      'article7',\n",
    "      'article8',\n",
    "      'article9',\n",
    "      'article10',\n",
    "      'article11',\n",
    "      'article12',\n",
    "      'article13',\n",
    "      'article14',\n",
    "      'article15',\n",
    "      'article16'\n",
    "      ]\n",
    "   \n",
    "    \n",
    "    \n",
    "    list3=['sci_space',\n",
    "'soc_religion_christian',\n",
    "'talk_politics_mideast',\n",
    "'talk_politics_guns',\n",
    "'soc_religion_christian',\n",
    "'talk_politics_guns',\n",
    "'talk_politics_mideast',\n",
    "'sci_med',\n",
    "'talk_politics_guns',\n",
    "'talk_politics_mideast',\n",
    "'talk_politics_mideast',\n",
    "'talk_politics_mideast',\n",
    "'talk_politics_mideast',\n",
    "'talk_politics_misc',\n",
    "'talk_politics_mideast'\n",
    "]\n",
    "    \n",
    "    list5=['20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20181203','20180101']\n",
    "    \n",
    "    list2=[]\n",
    "    list4=[]\n",
    "    list6=[]\n",
    "    \n",
    "    \n",
    "    #for i in range(len(list3)):\n",
    "       # list2.append(switcher(list3[i]))\n",
    "        \n",
    "    for i in list1:\n",
    "        list2.append(rdflib.URIRef('http://www.example.org/'+i))\n",
    "    '''\n",
    "    ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
    "    '''\n",
    "    for i in predict_result:\n",
    "        \"\"\"if i=='alt.atheism':\n",
    "            list4.append(rdflib.URIRef('http://www.example.org/alt_atheism'))\n",
    "        elif i=='comp.graphics':\n",
    "            list4.append(rdflib.URIRef('http://www.example.org/comp.graphics'))\n",
    "        elif i=='comp.os.ms-windows.misc':\n",
    "            list4.append(rdflib.URIRef('http://www.example.org/comp.os.ms-windows.misc'))\n",
    "        elif i=='talk_politics_guns':\n",
    "            list4.append(rdflib.URIRef('http://www.example.org/talk_politics_guns'))\n",
    "        elif i=='talk_politics_misc':\n",
    "            list4.append(rdflib.URIRef('http://www.example.org/talk_politics_misc'))\n",
    "        elif i=='sci_med':\n",
    "            list4.append(rdflib.URIRef('http://www.example.org/sci_med'))\n",
    "            \"\"\"\n",
    "        list4.append(rdflib.URIRef('http://www.example.org/'+i))\n",
    "            \n",
    "    for i in test_news_date:\n",
    "        list6.append(rdflib.URIRef('http://www.example.org/'+i))\n",
    "       \n",
    "    \n",
    "    #g.add((article1, belongs_class_of, sci_space))\n",
    "    #print(list2)\n",
    "    for i in range(len(list1)):\n",
    "         g.add((list2[i],belongs_class_of,list4[i]))\n",
    "    for i in range(len(list1)):       \n",
    "         g.add((list2[i],published_at,list6[i]))\n",
    "    ##############\n",
    "    # c3,has,c4\n",
    "    # c3,loc,p2\n",
    "    g.serialize(\"graph.rdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 results found\n",
      "Result 1 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article11'),)\n",
      "Result 2 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article3'),)\n",
      "Result 3 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article13'),)\n",
      "Result 4 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article5'),)\n",
      "Result 5 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article12'),)\n",
      "Result 6 of 6 \n",
      "(rdflib.term.URIRef('http://www.example.org/article15'),)\n"
     ]
    }
   ],
   "source": [
    "def query():\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(\"graph.rdf\", format=\"xml\")\n",
    "    ################################\n",
    "    # <a,?,?>\n",
    "    q = \"select ?article  where {?article <http://www.example.org/belongs_class_of>  <http://www.example.org/rec.sport.hockey> }\"\n",
    "    x = g.query(q)\n",
    "    t = list(x) \n",
    "    print(len(t),'results found')\n",
    "    ##### 二维\n",
    "    # print(t[0][0])\n",
    "    # http://www.example.org/has_border_with\n",
    "    # print(t[0][1])\n",
    "    # http://www.example.org/part1\n",
    "    for i in range(len(t)):\n",
    "        print(\"Result {} of {} \".format(i+1,len(t)))\n",
    "        print(t[i])\n",
    "    # <?,b,?>\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    create()\n",
    "    query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "pyml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
