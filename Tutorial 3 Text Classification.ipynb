{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 : Text classification using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 : Print the categories names and some text examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: eliot@lanmola.engr.washington.edu (eliot)\n",
      "Subject: Re: Improvements in Automatic Transmissions\n",
      "Keywords: Saturn, Subaru, manual, automatic\n",
      "Article-I.D.: engr.Apr19.045221.19525\n",
      "Organization: clearer than blir\n",
      "Lines: 40\n",
      "NNTP-Posting-Host: lanmola.engr.washington.edu\n",
      "\n",
      "\n",
      "an excellent automatic can be found in the subaru legacy.  it switches to\n",
      "\"sport\" mode when the electronics figure it,  not when the driver sets\n",
      "the switch.. which is the proper way to do it, IMO.  so what does \"sport\"\n",
      "mode entail?  several things:\n",
      "\n",
      "1) revving to red line (or to the rev limiter in the case of the legacy)\n",
      "\n",
      "2) delayed upshifts.  (i.e. if you lift off briefly, it will remain in the\n",
      "\tlow gear.  this is handy if you are charging through corners and\n",
      "\twould like to do without the distraction of upshifts when there's\n",
      "\tanother curve approaching)\n",
      "\n",
      "3) part throttle downshifts, based on the *speed* at which the pedal is\n",
      "\tdepressed, rather than the *position* of the pedal.  modern\n",
      "\telectronics can measure this very easily and switch to sport mode.\n",
      "\tthis is wonderful if you want to charge through a green light about\n",
      "\tto turn red.  my audi senses this very well and can downshift on as\n",
      "\tlittle as half throttle if my right foot is fast enough.\n",
      "\n",
      "also, i think that a smart automatic can deliver better gas mileage\n",
      "than a dumb driver with a stick, all else being equal.. remember that\n",
      "the idea of a stick being more economical than an automatic makes a\n",
      "big assumption that the driver is smart enough to know what gear to\n",
      "use for each situation.. how many times have you ridden with an\n",
      "inattentive driver cruising on the highway at 55/65 in 4th gear (of a\n",
      "5 speed)?  \n",
      "\n",
      "how many % of people who drive manuals *really* know what the best\n",
      "gear to use is for every conceivable situation?  i'm sure there will\n",
      "be some who know, but i suspect that a chip controlled automatic with\n",
      "all possible scenario/ratio combinations stored in ROM is likely to do\n",
      "better.  i can also say that all my previous assumptions were proved\n",
      "wrong after i got a car with instantaneous mpg readout... high gear,\n",
      "low revs and wide open throttle is more economical than low gear, high\n",
      "revs and small throttle opening.  the explanation is quite simple if\n",
      "one sits down to think about it, but not that obvious at first sight.\n",
      "\n",
      "\n",
      "eliot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can check the target names (categories) by using target_names\n",
    "# the texts are stored in the data attribute of the variable twinty train\n",
    "print(twenty_train.target_names)\n",
    "print(twenty_train.data[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features extraction from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the scikit documentation to see definition of CoutVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "#feature_name = count_vect.get_feature_names()\n",
    "#print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF is an other text preprocessing procedure : Check its definition and apply it using the following code TF-IDF处理\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类器的相关信息：\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "print(\"分类器的相关信息：\")\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline including preprocessings and learning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# We will be using the 'text_clf' going forward.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice : evaluate the performance of the first model (named clf) on the 20 newsgroups test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朴素贝叶斯准确率为：\n",
      "0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "# 计算预测结果的准确率\n",
    "print(\"朴素贝叶斯准确率为：\")\n",
    "print(np.mean(predicted ==twenty_test.target))\n",
    "#print(test_data.target)\n",
    "#print(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice : Using scikit pipeline and SVM Classifier (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html), create a model based on the 20newsgroup dataset and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haosu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM准确率：\n",
      "0.8238183749336165\n",
      "打印分类性能指标：\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.52      0.63       319\n",
      "           comp.graphics       0.81      0.65      0.72       389\n",
      " comp.os.ms-windows.misc       0.82      0.65      0.73       394\n",
      "comp.sys.ibm.pc.hardware       0.67      0.78      0.72       392\n",
      "   comp.sys.mac.hardware       0.86      0.77      0.81       385\n",
      "          comp.windows.x       0.89      0.75      0.82       395\n",
      "            misc.forsale       0.93      0.69      0.80       390\n",
      "               rec.autos       0.85      0.92      0.88       396\n",
      "         rec.motorcycles       0.94      0.93      0.93       398\n",
      "      rec.sport.baseball       0.92      0.90      0.91       397\n",
      "        rec.sport.hockey       0.89      0.97      0.93       399\n",
      "               sci.crypt       0.59      0.97      0.74       396\n",
      "         sci.electronics       0.84      0.60      0.70       393\n",
      "                 sci.med       0.92      0.74      0.82       396\n",
      "               sci.space       0.84      0.89      0.87       394\n",
      "  soc.religion.christian       0.44      0.98      0.61       398\n",
      "      talk.politics.guns       0.64      0.94      0.76       364\n",
      "   talk.politics.mideast       0.93      0.91      0.92       376\n",
      "      talk.politics.misc       0.96      0.42      0.58       310\n",
      "      talk.religion.misc       0.97      0.14      0.24       251\n",
      "\n",
      "               micro avg       0.77      0.77      0.77      7532\n",
      "               macro avg       0.83      0.76      0.76      7532\n",
      "            weighted avg       0.82      0.77      0.77      7532\n",
      "\n",
      "打印混淆矩阵：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[166,   0,   0,   1,   0,   1,   0,   0,   1,   1,   1,   3,   0,\n",
       "          6,   3, 123,   4,   8,   0,   1],\n",
       "       [  1, 252,  15,  12,   9,  18,   1,   2,   1,   5,   2,  41,   4,\n",
       "          0,   6,  15,   4,   1,   0,   0],\n",
       "       [  0,  14, 258,  45,   3,   9,   0,   2,   1,   3,   2,  25,   1,\n",
       "          0,   6,  23,   2,   0,   0,   0],\n",
       "       [  0,   5,  11, 305,  17,   1,   3,   6,   1,   0,   2,  19,  13,\n",
       "          0,   5,   3,   1,   0,   0,   0],\n",
       "       [  0,   3,   8,  23, 298,   0,   3,   8,   1,   3,   1,  16,   8,\n",
       "          0,   2,   8,   3,   0,   0,   0],\n",
       "       [  1,  21,  17,  13,   2, 298,   1,   0,   1,   1,   0,  23,   0,\n",
       "          1,   4,  10,   2,   0,   0,   0],\n",
       "       [  0,   1,   3,  31,  12,   1, 271,  19,   4,   4,   6,   5,  12,\n",
       "          6,   3,   9,   3,   0,   0,   0],\n",
       "       [  0,   1,   0,   3,   0,   0,   4, 364,   3,   2,   2,   4,   1,\n",
       "          1,   3,   3,   4,   0,   1,   0],\n",
       "       [  0,   0,   0,   1,   0,   0,   2,  10, 371,   0,   0,   4,   0,\n",
       "          0,   0,   8,   2,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   1,   0,   0,   4,   0, 357,  22,   0,   0,\n",
       "          0,   2,   9,   1,   1,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   1,   0,   4, 387,   1,   0,\n",
       "          0,   1,   5,   0,   0,   0,   0],\n",
       "       [  0,   2,   1,   0,   0,   1,   1,   3,   0,   0,   0, 383,   1,\n",
       "          0,   0,   3,   1,   0,   0,   0],\n",
       "       [  0,   4,   2,  17,   5,   0,   2,   8,   7,   1,   2,  78, 235,\n",
       "          3,  11,  15,   2,   1,   0,   0],\n",
       "       [  2,   3,   0,   1,   1,   3,   1,   0,   2,   3,   4,  11,   5,\n",
       "        292,   6,  52,   6,   4,   0,   0],\n",
       "       [  0,   2,   0,   1,   0,   3,   0,   2,   1,   0,   1,   6,   1,\n",
       "          2, 351,  19,   4,   0,   1,   0],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,\n",
       "          1,   2, 392,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   1,   0,   0,   2,   0,   1,   1,   0,  10,   0,\n",
       "          0,   1,   6, 341,   1,   0,   0],\n",
       "       [  0,   1,   0,   0,   0,   0,   0,   0,   0,   1,   0,   2,   0,\n",
       "          0,   0,  24,   3, 344,   1,   0],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,   1,   0,   0,   1,  11,   0,\n",
       "          1,   7,  35, 118,   5, 129,   0],\n",
       "       [ 33,   2,   0,   0,   0,   0,   0,   0,   0,   1,   1,   3,   0,\n",
       "          4,   4, 131,  29,   5,   3,  35]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge',\n",
    "                                            penalty='l2',\n",
    "                                            alpha=1e-3,\n",
    "                                            n_iter=5,\n",
    "                                            random_state=42)),\n",
    "                    ])\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_SVM = text_clf.predict(twenty_test.data)\n",
    "print(\"SVM准确率：\")\n",
    "print(np.mean(predicted_SVM == twenty_test.target))\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"打印分类性能指标：\")\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "print(\"打印混淆矩阵：\")\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search : Trying different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name.\n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haosu/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/haosu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the best mean score and the params, run the following code\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "gs_clf.best_params_\n",
    "# 详细 gs_clf.cv_results_\n",
    "\n",
    "# Output for above should be: The accuracy has now increased to ~90.6% for the NB classifier\n",
    "# and the corresponding parameters are {‘clf__alpha’: 0.01, ‘tfidf__use_idf’: True, ‘vect__ngram_range’: (1, 2)}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for SVM : Similarly, do grid search for SVM and output the best score and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haosu/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/haosu/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/haosu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:130: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳准确率：0.6025\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n",
    "print(\"最佳准确率：%r\" % (gs_clf.best_score_))\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming help you improve your model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal is also an other technique used in text classification in order to improve the model performances : Take a look at this webpage for more information : https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(twenty_train.data))\n",
    "dataBank = []\n",
    "\n",
    "for article in twenty_train.data:\n",
    "    dataBank.append(word_tokenize(article))\n",
    "\n",
    "\n",
    "# print(word_tokenize(twenty_train.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK process\n",
    "stop_words.add(\"I\")\n",
    "stop_words.add(\",\")\n",
    "stop_words.add(\"@\")\n",
    "stop_words.add(\"(\")\n",
    "stop_words.add(\")\")\n",
    "stop_words.add(\"-\")\n",
    "stop_words.add(\"_\")\n",
    "stop_words.add(\".\")\n",
    "stop_words.add(\":\")\n",
    "stop_words.add(\"A\")\n",
    "stop_words.add(\"<\")\n",
    "stop_words.add(\">\")\n",
    "stop_words.add(\"n't\")\n",
    "stop_words.add(\"'s\")\n",
    "stop_words.add(\"?\")\n",
    "stop_words.add(\"!\")\n",
    "stop_words.add(\"--\")\n",
    "twenty_train_split = []\n",
    "for i in range(len(dataBank)):\n",
    "    filtered_sentence = []\n",
    "    for word in dataBank[i]:\n",
    "        if word not in stop_words:\n",
    "            filtered_sentence.append(word)\n",
    "    twenty_train_split.append(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-ce5a31080084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtwenty_train_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtwenty_train_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in len(twenty_train_split):\n",
    "    twenty_train_split[i] = \" \".join( twenty_train_split[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-dd3ed5bf4632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "print(\" \".join(twenty_train_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love','stage']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for i in predicted:\n",
    "    print(twenty_train.target_names[i])\n",
    "#for doc,category in zip(docs_new,predicted):\n",
    "    #print(\"%r => %s\") %(doc,twenty_train.target_names[category])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyml",
   "language": "python",
   "name": "pyml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
